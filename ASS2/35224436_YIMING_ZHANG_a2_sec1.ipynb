{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d82c02",
   "metadata": {},
   "source": [
    "# 1 Document Clustering\n",
    "Student ID: 35224436 | Full name: Yiming Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1515faee",
   "metadata": {},
   "source": [
    "## Question 1 Expectation Maximisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12617adb",
   "metadata": {},
   "source": [
    "### Task I\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5c068",
   "metadata": {},
   "source": [
    "#### Write mathematical formulations of the optimization functions of maximum likelihood estimation (MLE) for the document clustering model with complete data and incomplete data, respectively. Then briefly describe why MLE with incomplete data is hard to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6347821",
   "metadata": {},
   "source": [
    "##### <font color=red><b>Multiple documents MLE (complete data)</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815f96f",
   "metadata": {},
   "source": [
    "For each document ${d_n}$, choose a cluster $k \\in \\{1, \\dots, K\\}$ by tossing a $K$-faced dice parameterized by $\\boldsymbol{\\varphi} = [\\varphi_1,\\dots, \\varphi_K]$:\n",
    "$$\n",
    "p(z_n = k) = \\varphi_k, \\quad \\sum_{k=1}^{K} \\varphi_k = 1\n",
    "$$\n",
    "where $\\varphi_k$ represents the prior probability that document $d_n$ belongs to cluster $k$.\n",
    "\n",
    "Given the selected cluster $k$,\n",
    "generate each word in the document independently from the word distribution $\\boldsymbol{\\mu}_k = [\\mu_{k,w}]_{w \\in \\mathcal{A}}$:\n",
    "\n",
    "$$\n",
    "p(w \\mid z_n = k) = \\mu_{k,w}, \\quad \n",
    "\\sum_{w \\in \\mathcal{A}} \\mu_{k,w} = 1\n",
    "$$\n",
    "\n",
    "Each $\\boldsymbol{\\mu}_k$ corresponds to the multinomial word distribution for cluster $k$.\n",
    "\n",
    "Thus, the joint probability of a document $d$ and its cluster $k$ is:\n",
    "\n",
    "$$\n",
    "p(d, k) = p(k) \\, p(d \\mid k)\n",
    "= \\varphi_k \\prod_{w \\in \\mathcal{A}} \\mu_{k,w}^{\\,c(w, d)}\n",
    "$$\n",
    "\n",
    "where $c(w, d)$ is the number of times word $w$ appears in document $d$.\n",
    "\n",
    "Assume that in addition to the documents $D = \\{{d_1, d_2, ..., d_N}\\}$, each document $d_N$ has its own topic label $\\{{k_1, k_2, ..., k_N}\\}$. We can represent the joint probalitity as:\n",
    "\n",
    "$$\n",
    "p(d_1, k_1,...,d_N,k_N) = p(d_1, k_1) · p(d_2, k_2) · ... · p(d_N, k_N)\n",
    "= \\prod_{n=1}^{N} p(k_N)\\,p(d_N \\mid k_N)\n",
    "$$\n",
    "\n",
    "Therefore, the general form is:\n",
    "\n",
    "$$\n",
    "p(d_1, z_1, \\dots, d_N, z_N)\n",
    "= \\prod_{n=1}^{N} \\prod_{k=1}^{K}\n",
    "\\left(\n",
    "\\varphi_k\n",
    "\\prod_{w \\in \\mathcal{A}} \n",
    "\\mu_{k,w}^{\\,c(w, d_n)}\n",
    "\\right)^{z_{n,k}}\n",
    "$$\n",
    "\n",
    "where $z_{n,k} \\in \\{0,1\\}$ is an indicator variable that equals 1 if document $d_n$ belongs to cluster $k$, and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb49fb",
   "metadata": {},
   "source": [
    "##### <font color=red><b>Multiple documents MLE (incomplete data)</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569682c9",
   "metadata": {},
   "source": [
    "In this case, we can only observe the document $d$, but we do not know which cluster $k$ it belongs to. The cluster assignment $k$ is therefore a latent variable. Hence, we marginalize over $k$ to eliminate this unobserved variable.\n",
    "\n",
    "Therefore, we can represent the joint probability of a document $d$ as:\n",
    "\n",
    "$$\n",
    "p(d) = \\sum_{k}p(d, k) = \\sum_{k} \\varphi_{k} · p(d \\mid k)\n",
    "= \\sum_{k} (\\varphi_k \\prod_{w \\in \\mathcal{A}} \\mu_{k, w}^{\\,c(w, d)})\n",
    "$$\n",
    "\n",
    "where $c(w, d)$ is the number of times word $w$ appears in document $d$.\n",
    "\n",
    "Assume that in addition to the documents $D = \\{{d_1, d_2, ..., d_N}\\}$, $z_n$ is a latent variable. We can represent the joint probalitity as:\n",
    "\n",
    "$$\n",
    "p(d_1, \\dots, d_N)\n",
    "= \\prod_{n=1}^{N} p(d_n)\n",
    "= \\prod_{n=1}^{N} \\sum_{k=1}^{K} p(z_{n,k} = 1, d_n)\n",
    "$$\n",
    "\n",
    "Here, $z_{n,k}=1$ indicates that document $d_n$ belongs to cluster $k$, and $z_{n,k}=0$ otherwise.\n",
    "\n",
    "Substituting the generative model, we obtain the general form:\n",
    "\n",
    "$$\n",
    "p(d_1, \\dots, d_N) =\n",
    "= \\prod_{n=1}^{N} \\sum_{k=1}^{K}\n",
    "\\left(\n",
    "\\varphi_k \n",
    "\\prod_{w \\in \\mathcal{A}}\n",
    "\\mu_{k,w}^{\\,c(w, d_n)}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Taking the logarithm gives the incomplete-data log-likelihood  \n",
    "\n",
    "$$\n",
    "\\ln {p(d_1, z_1, \\dots, d_N, z_N)}\n",
    "= \\sum_{n=1}^{N} \\ln{p(d_n)}\n",
    " = \\sum_{n=1}^{N}\n",
    "   \\ln \\,\n",
    "      \\sum_{k=1}^{K}\n",
    "      \\left(\\varphi_k\n",
    "      \\prod_{w \\in \\mathcal{A}}\n",
    "      \\mu_{k,w}^{\\,c(w,d_n)}\\right)\n",
    "$$\n",
    "\n",
    "MLE with incomplete data is hard to optimize because the log-likelihood contains\n",
    "a logarithm of a summation over hidden clusters.  \n",
    "The latent variables $z_n$ are unobserved, so the parameters\n",
    "$\\varphi_k$ and $\\mu_{k,w}$ are coupled inside the logarithm,\n",
    "preventing closed-form analytical updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa20738",
   "metadata": {},
   "source": [
    "#### Briefly explain the high-level idea of the EM algorithm to find MLE parameter estimates.\n",
    "\n",
    "Since the usual MLE on the incomplete-data log-likelihood is hard, EM tackles this by alternating two steps on a current parameter guess.\n",
    "\n",
    "In the **E-step**, we compute the expected value of the complete-data log-likelihood with respect to the posterior distribution of the latent cluster assignments under the current parameter estimates $\\boldsymbol{\\theta}^{\\text{old}}$. \n",
    "\n",
    "This gives the **Q function**, which forms the basis of the EM algorithm:\n",
    "\n",
    "$$\n",
    "Q(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}) \n",
    "= \\sum_{n=1}^{N} \\sum_{k=1}^{K} \n",
    "p(z_{n,k}=1 \\mid d_n, \\boldsymbol{\\theta}^{\\text{old}}) \n",
    "\\ln p(z_{n,k}=1, d_n \\mid \\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "Substituting the generative model of document clustering, we get:\n",
    "\n",
    "$$\n",
    "Q(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}) \n",
    "= \\sum_{n=1}^{N} \\sum_{k=1}^{K} \n",
    "\\gamma(z_n, k)\n",
    "\\left(\n",
    "\\ln \\varphi_k +\n",
    "\\sum_{w \\in \\mathcal{A}} c(w, d_n)\\ln \\mu_{k,w}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "where  \n",
    "\n",
    "- $\\boldsymbol{\\theta} = (\\boldsymbol{\\varphi}, \\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_K)$  \n",
    "  is the collection of model parameters.  \n",
    "- $\\gamma(z_n, k) = p(z_{n,k}=1 \\mid d_n, \\boldsymbol{\\theta}^{\\text{old}})$  \n",
    "  represents the **responsibility** (posterior probability) that document $d_n$ belongs to cluster $k$.  \n",
    "- $c(w, d_n)$ is the number of occurrences of word $w$ in document $d_n$.  \n",
    "\n",
    "In the **M-step**, we maximize this $Q$ function with respect to $\\boldsymbol{\\theta}$, which yields the updated parameter estimates:\n",
    "\n",
    "$$\n",
    "\\varphi_k^{\\text{new}} = \\frac{N_k}{N}, \n",
    "\\quad \n",
    "N_k = \\sum_{n=1}^{N}\\gamma(z_n, k)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_{k,w}^{\\text{new}} =\n",
    "\\frac{\\sum_{n=1}^{N}\\gamma(z_n, k)c(w, d_n)}\n",
    "     {\\sum_{w'\\in\\mathcal{A}}\\sum_{n=1}^{N}\\gamma(z_n, k)c(w', d_n)}\n",
    "$$\n",
    "\n",
    "Each EM iteration alternates between estimating the posterior responsibilities (E-step) and maximizing the expected complete-data log-likelihood (M-step). This process guarantees that the incomplete-data log-likelihood **never decreases** across iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909bed2",
   "metadata": {},
   "source": [
    "### Task II\n",
    "#### Problem Setup\n",
    "Given a collection of documents $D = \\{{d_1,d_2,...,d_N}\\}$ with vocabulary $\\mathcal{A}$, we want to cluster them into $K$ clusters.\n",
    "\n",
    "#### Model Parameters\n",
    "The parameters to learn are:\n",
    "$$\n",
    "\\boldsymbol{\\theta} := (\\boldsymbol{\\varphi}, \\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_K)\n",
    "$$\n",
    "where:\n",
    "- **Mixing coefficients:** $\\boldsymbol{\\varphi} = (\\varphi_1, \\ldots, \\varphi_K)$ with $\\sum_{k=1}^K \\varphi_k = 1$ and $\\varphi_k \\geq 0$\n",
    "\n",
    "- **Word distributions:** $\\boldsymbol{\\mu}_k = (\\mu_{k,w})_{w \\in \\mathcal{A}}$​ for cluster $k$, with $\\sum_{w \\in \\mathcal{A}} \\mu_{k,w} = 1$ and $\\mu_{k,w} \\geq 0$\n",
    "\n",
    "#### Latent Variables\n",
    "\n",
    "$z_n​=(z_{n,1}​,…,z_{n,K}​)$ where $z_{n,k}=1$ if document $d_n$​ belongs to cluster $k$, and 0 otherwise.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "$c(w,d_n​) =$ count of word $w$ in document $d_n$\n",
    "\n",
    "#### Derive process\n",
    "\n",
    "First, random generate initial parameters: $\\mu_{old}$ and $\\varphi_{old}$.\n",
    "\n",
    "##### E-step\n",
    "\n",
    "In E-step, for every $d_n$ and every cluster $k$, compute the posterior probability:\n",
    "\n",
    "$$\n",
    "\\gamma(z_n, k) = p(z_{n,k}=1 \\mid d_n, \\boldsymbol{\\theta}^{\\text{old}})\n",
    "$$\n",
    "\n",
    "According to the Bayes Rules:\n",
    "\n",
    "$$\n",
    "\\gamma(z_n, k) = p(z_{n,k}=1 \\mid d_n, \\boldsymbol{\\theta}^{\\text{old}})\n",
    "= \\frac{p(z_n=k \\mid \\theta)\\,p(d_n \\mid z_n=k, \\theta)}{\\sum_{j=1}^{K}\\,p(z_n=j \\mid \\theta)\\,p(d_n \\mid z_n=j, \\theta)}\n",
    "$$\n",
    "\n",
    "Substituting $p(z_n=k \\mid \\theta)=\\mu_{k}$ and $p(d_n \\mid z_n=k, \\theta)= \\prod_{w \\in \\mathcal{A}} \\mu_{k,w}^{\\,c(w, d_n)}$,\n",
    "we have **E-Step Update Rule**:\n",
    "\n",
    "\n",
    "$$\n",
    "\\gamma(z_n, k) = \\frac{\\varphi_{old}\\,\\prod_{w \\in \\mathcal{A}}(\\mu_{k,w}^{old})^{\\,c(w, d_n)}}{\\sum_{j=1}^{K}\\,\\varphi_{j}^{old}\\,\\prod_{w \\in \\mathcal{A}}(\\mu_{j,w}^{old})^{c(w, d_n)}}\n",
    "$$\n",
    "\n",
    "\n",
    "In M-Step, we need maximize $Q(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}})$ subject to:\n",
    "$\\sum_{k=1}^{K} \\varphi_k = 1$ and $\\sum_{w \\in \\mathcal{A}} \\mu_{k,w} = 1$.\n",
    "\n",
    "Form the Lagrangian:\n",
    "$$\n",
    "\\mathcal{L} = Q(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}) + \\lambda \\left( \\sum_{k=1}^{K} \\varphi_k - 1 \\right)\n",
    "$$\n",
    "\n",
    "Taking derivative with respect to $\\varphi_k$​:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\varphi_k} = \\sum_{n=1}^{N} \\gamma(z_n, k) \\cdot \\frac{1}{\\varphi_k} + \\lambda = 0\n",
    "\\Rightarrow \\sum_{n=1}^{N} \\gamma(z_n, k) = -\\lambda \\varphi_k\n",
    "$$\n",
    "\n",
    "Summing over $k$:\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^{K} \\sum_{n=1}^{N} \\gamma(z_n, k) = -\\lambda \\sum_{k=1}^{K} \\varphi_k = -\\lambda\n",
    "$$\n",
    "\n",
    "Since $\\sum_{k=1}^{K} \\gamma(z_n, k) = 1$, we have $\\sum_{k=1}^{K} \\sum_{n=1}^{N} \\gamma(z_n, k) = N$.\n",
    "\n",
    "Therefore: $\\lambda = -N$. Substituting back we have **M-Step Update Rule for $\\varphi_k$**:\n",
    "\n",
    "$$\n",
    "\\varphi_k^{\\text{new}} = \\frac{1}{N} \\sum_{n=1}^{N} \\gamma(z_n, k) = \\frac{N_k}{N}\n",
    "$$\n",
    "\n",
    "where $N_k := \\sum_{n=1}^{N} \\gamma(z_n, k)$ is the effective number of documents assigned to cluster $k$.\n",
    "\n",
    "\n",
    "##### M-step\n",
    "Then we need derive the updating rule of Word Distributions $\\mu_{k,w}$:\n",
    "\n",
    "For each cluster $k$, form the Lagrangian:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_k = Q(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\text{old}}) + \\lambda_k \\left( \\sum_{w \\in \\mathcal{A}} \\mu_{k,w} - 1 \\right)\n",
    "$$\n",
    "\n",
    "Taking derivative with respect to $\\mu_{k,w}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_k}{\\partial \\mu_{k,w}} = \\sum_{n=1}^{N} \\gamma(z_n, k) \\cdot \\frac{c(w, d_n)}{\\mu_{k,w}} + \\lambda_k = 0\n",
    "$$\n",
    "$$\n",
    "\\Rightarrow \\sum_{n=1}^{N} \\gamma(z_n, k) \\cdot c(w, d_n) = -\\lambda_k \\mu_{k,w}\n",
    "$$\n",
    "\n",
    "Summing over $w \\in \\mathcal{A}$:\n",
    "\n",
    "$$\n",
    "\\sum_{w \\in \\mathcal{A}} \\sum_{n=1}^{N} \\gamma(z_n, k) \\cdot c(w, d_n) = -\\lambda_k \\sum_{w \\in \\mathcal{A}} \\mu_{k,w} = -\\lambda_k\n",
    "$$\n",
    "\n",
    "The left side equals:\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^{N} \\gamma(z_n, k) \\sum_{w \\in \\mathcal{A}} c(w, d_n) = \\sum_{n=1}^{N} \\gamma(z_n, k) \\cdot |d_n|\n",
    "$$\n",
    "\n",
    "where $|d_n|$ is the total word count in document $d_n$.\n",
    "\n",
    "For simplicity, let:\n",
    "\n",
    "$$\n",
    "\\lambda_k = -\\sum_{w' \\in \\mathcal{A}} \\sum_{n=1}^{N} \\gamma(z_n, k) \\cdot c(w', d_n)\n",
    "$$\n",
    "\n",
    "Then we get the **M-Step Update Rule for $\\mu_{k,w}$**:\n",
    "\n",
    "$$\n",
    "\\mu_{k,w}^{\\text{new}} = \\frac{\\sum_{n=1}^{N} \\gamma(z_n, k) \\cdot c(w, d_n)}{\\sum_{w' \\in \\mathcal{A}} \\sum_{n=1}^{N} \\gamma(z_n, k) \\cdot c(w', d_n)}\n",
    "$$\n",
    "\n",
    "##### Assignment (After Convergence):\n",
    "Assign document $d_n$ to cluster:\n",
    "$$\n",
    "k^* = \\arg\\max_{k} \\gamma(z_n, k)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e4f2ca",
   "metadata": {},
   "source": [
    "### Task III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29576f7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data_S2_2025/Task2A.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Data_S2_2025/Task2A.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mall\u001b[39m([length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m length \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mlen\u001b[39m(line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m text]])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data_S2_2025/Task2A.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "with open('Assignment/ML_ASS/ASS2/Dataset_S2_2025/Task2A.txt', 'r') as file:\n",
    "    text = file.readlines()\n",
    "all([length == 2 for length in [len(line.split('\\t')) for line in text]])\n",
    "labels, articles = [line.split('\\t')[0].strip() for line in text], [line.split('\\t')[1].strip() for line in text]\n",
    "docs = pd.DataFrame(data = zip(labels,articles), columns=['label', 'article'])\n",
    "docs.label = docs.label.astype('category')\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d42b12",
   "metadata": {},
   "source": [
    "### Task IV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724d8ff0",
   "metadata": {},
   "source": [
    "### Task V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d30c65",
   "metadata": {},
   "source": [
    "### Task VI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
