{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f51cc77",
   "metadata": {},
   "source": [
    "## 2 Ridge Regression\n",
    "### Student ID: 35224436 | Full name: Yiming Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76c70fc",
   "metadata": {},
   "source": [
    "## Task I.  SGD Weight Update Derivation\n",
    "\n",
    "#### 1. Regularized Error Function Definition\n",
    "\n",
    "$$\n",
    "E(\\mathbf{w}) = \\frac{1}{2}\\sum_{n=1}^{N}(y_n - \\mathbf{w}^T\\mathbf{x}_n)^2 + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^D$ is the weight vector\n",
    "- $\\mathbf{x}_n \\in \\mathbb{R}^D$ is the $n$-th input feature vector\n",
    "- $y_n \\in \\mathbb{R}$ is the $n$-th target value\n",
    "- $\\lambda > 0$ is the regularization parameter\n",
    "- $\\|\\mathbf{w}\\|^2 = \\mathbf{w}^T\\mathbf{w} = \\sum_{j=1}^{D}w_j^2$ is the squared L2 norm\n",
    "\n",
    "#### 2. Gradient Computation\n",
    "\n",
    "To use gradient descent, we need to compute the gradient of the error function with respect to the weight vector $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} E(\\mathbf{w}) = \\frac{\\partial E(\\mathbf{w})}{\\partial \\mathbf{w}}\n",
    "$$\n",
    "\n",
    "Computing the gradients of the error term and regularization term separately:\n",
    "\n",
    "**Gradient of the error term:**\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}} \\left[\\frac{1}{2}\\sum_{n=1}^{N}(y_n - \\mathbf{w}^T\\mathbf{x}_n)^2\\right] = -\\sum_{n=1}^{N}(y_n - \\mathbf{w}^T\\mathbf{x}_n)\\mathbf{x}_n\n",
    "$$\n",
    "\n",
    "**Gradient of the regularization term:**\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}} \\left[\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2\\right] = \\frac{\\lambda}{2} \\cdot 2\\mathbf{w} = \\lambda\\mathbf{w}\n",
    "$$\n",
    "\n",
    "**Total gradient:**\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} E(\\mathbf{w}) = -\\sum_{n=1}^{N}(y_n - \\mathbf{w}^T\\mathbf{x}_n)\\mathbf{x}_n + \\lambda\\mathbf{w}\n",
    "$$\n",
    "\n",
    "#### 3. SGD Update\n",
    "\n",
    "In stochastic gradient descent, we use only one sample $(x_n, y_n)$ at a time to update the weights. For a single sample, the error function is:\n",
    "\n",
    "$$\n",
    "E_n(\\mathbf{w}) = \\frac{1}{2}(y_n - \\mathbf{w}^T\\mathbf{x}_n)^2 + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2\n",
    "$$\n",
    "\n",
    "The gradient for a single sample is:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} E_n(\\mathbf{w}) = -(y_n - \\mathbf{w}^T\\mathbf{x}_n)\\mathbf{x}_n + \\lambda\\mathbf{w}\n",
    "$$\n",
    "\n",
    "**SGD weight update rule:**\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(t)} = \\mathbf{w}^{(t-1)} - \\eta \\nabla_{\\mathbf{w}} E_n(\\mathbf{w}^{(t-1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(t)} = \\mathbf{w}^{(t-1)} - \\eta[-(y_n - \\mathbf{w}^{(t-1)T}\\mathbf{x}_n)\\mathbf{x}_n + \\lambda\\mathbf{w}^{(t-1)}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(t)} = \\mathbf{w}^{(t-1)} + \\eta(y_n - \\mathbf{w}^{(t-1)T}\\mathbf{x}_n)\\mathbf{x}_n - \\eta\\lambda\\mathbf{w}^{(t-1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(t)} = \\mathbf{w}^{(t-1)}(1 - \\eta\\lambda) + \\eta(y_n - \\mathbf{w}^{(t-1)T}\\mathbf{x}_n)\\mathbf{x}_n\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\eta > 0$ is the learning rate\n",
    "- $t$ is the iteration number\n",
    "- $(x_n, y_n)$ is the current training sample\n",
    "\n",
    "#### 4. Matrix/Vector Representation\n",
    "\n",
    "The above update rule can be written in matrix form as:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)}(1 - \\eta\\lambda) + \\eta\\mathbf{x}_n(y_n - \\mathbf{x}_n^T\\mathbf{w}^{(t)})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397cf942",
   "metadata": {},
   "source": [
    "## Task II. SGD Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77189ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGDLinearRegressor:\n",
    "    def __init__(self, batch_size=1, eta=0.01, tau_max=1000, epsilon=0.00001, random_state=None, lam=1):\n",
    "        self.eta = eta\n",
    "        self.tau_max = tau_max\n",
    "        self.epsilon = epsilon\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.lam = lam\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        RNG = np.random.default_rng(self.random_state)\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        \n",
    "        # Ensure y is 1D\n",
    "        if y.ndim > 1:\n",
    "            y = y.flatten()\n",
    "        \n",
    "        n, p = x.shape\n",
    "        print(f\"Training data shape: x={x.shape}, y={y.shape}, features={p}\")\n",
    "\n",
    "        self.w_ = np.zeros(shape=(self.tau_max + 1, p))\n",
    "        w_prev = self.w_[0].copy()  # Make a copy to avoid reference issues\n",
    "\n",
    "        for tau in range(1, self.tau_max + 1):\n",
    "            idx = RNG.choice(n, size=self.batch_size, replace=True)\n",
    "            Xb, yb = x[idx], y[idx]\n",
    "\n",
    "            # Compute predictions and residuals\n",
    "            pred = Xb.dot(w_prev)  # Shape: (batch_size,)\n",
    "            resid = pred - yb      # Shape: (batch_size,)\n",
    "            \n",
    "            # Compute gradients\n",
    "            grad_data = Xb.T.dot(resid) / self.batch_size  # Shape: (p,)\n",
    "            grad_reg = self.lam * w_prev                   # Shape: (p,)\n",
    "            grad = grad_data + grad_reg                    # Shape: (p,)\n",
    "            \n",
    "            # Update weights\n",
    "            w_new = w_prev - self.eta * grad  # Shape: (p,)\n",
    "            \n",
    "            # Debug: Print shapes for first iteration\n",
    "            if tau == 1:\n",
    "                print(f\"Debug shapes:\")\n",
    "                print(f\"  Xb: {Xb.shape}, yb: {yb.shape}\")\n",
    "                print(f\"  pred: {pred.shape}, resid: {resid.shape}\")\n",
    "                print(f\"  grad_data: {grad_data.shape}, grad_reg: {grad_reg.shape}\")\n",
    "                print(f\"  grad: {grad.shape}, w_new: {w_new.shape}\")\n",
    "                print(f\"  w_[tau]: {self.w_[tau].shape}\")\n",
    "\n",
    "            self.w_[tau] = w_new\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(w_new - w_prev) < self.epsilon:\n",
    "                print(f\"Converged at iteration {tau}\")\n",
    "                break\n",
    "                \n",
    "            w_prev = w_new.copy()  # Make a copy for next iteration\n",
    "\n",
    "        self.coef_ = self.w_[tau]\n",
    "        self.w_ = self.w_[:tau + 1]\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        return x.dot(self.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162618eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGDLinearRegressor:\n",
    "    def __init__(self, batch_size=1, eta=0.01, tau_max=1000, epsilon=0.00001, random_state=None, lam=1):\n",
    "        self.eta = eta\n",
    "        self.tau_max = tau_max\n",
    "        self.epsilon = epsilon\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.lam = lam\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        RNG = np.random.default_rng(self.random_state)\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        if y.ndim > 1:\n",
    "            y = y.flatten()\n",
    "        n, p = x.shape\n",
    "\n",
    "        self.w_ = np.zeros(shape=(self.tau_max + 1, p))\n",
    "        w_prev = self.w_[0]\n",
    "\n",
    "        for tau in range(1, self.tau_max + 1):\n",
    "            idx = RNG.choice(n, size=self.batch_size, replace=True)\n",
    "            Xb, yb = x[idx], y[idx]\n",
    "\n",
    "            resid = Xb.dot(w_prev) - yb      # residual\n",
    "            grad_data = Xb.T.dot(resid) / self.batch_size\n",
    "            grad = grad_data + self.lam * w_prev\n",
    "\n",
    "            w_new = w_prev - self.eta * grad\n",
    "\n",
    "            self.w_[tau] = w_new\n",
    "            if np.linalg.norm(w_new - w_prev) < self.epsilon:\n",
    "                break\n",
    "            w_prev = w_new\n",
    "\n",
    "        self.coef_ = self.w_[tau]\n",
    "        self.w_ = self.w_[:tau + 1]\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        return x.dot(self.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8355ef",
   "metadata": {},
   "source": [
    "## Task III: L2 Regularization Effect Analysis\n",
    "\n",
    "### Step 1: Synthetic data construction\n",
    "\n",
    "Dataset is generated according to the following specifications.\n",
    "\n",
    "- $X \\sim \\text{Uniform}(-0.6, 0.6)$\n",
    "- $Y = \\frac{\\sin(2\\pi x)}{2 + 3x} + \\epsilon$\n",
    "- $\\epsilon \\sim N(0, 0.1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# true function\n",
    "def f(x):\n",
    "    x = np.asarray(x)\n",
    "    return np.sin(2*np.pi*x)/(2+3*x)\n",
    "\n",
    "# generation function\n",
    "def make_additive_noise_data(n, f, a, b, noise=0.1, random_state=None):\n",
    "    RNG = np.random.default_rng(random_state)\n",
    "    x = RNG.uniform(a, b, size=(n, 1))\n",
    "    y = f(x) + RNG.normal(0, noise, size=(n, 1))\n",
    "    return x, y\n",
    "\n",
    "# Visualization function following Activity 2.3 style  \n",
    "def plot_function(f, a, b, models=[], data=None, ax=None, ax_labels=True, legend=True):\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "    xx = np.linspace(a, b, 200).reshape(-1, 1)\n",
    "    if len(models)==1:\n",
    "        ax.fill_between(xx.squeeze(), f(xx).squeeze(), models[0].predict(xx).squeeze(), alpha=0.3)\n",
    "        ax.plot(xx, models[0].predict(xx), label='$y$')\n",
    "    if len(models) > 1:\n",
    "        for model in models: ax.plot(xx, model.predict(xx), color='gray', alpha=0.5)\n",
    "    ax.plot(xx, f(xx), color='black', label='$f$')\n",
    "    if data is not None:\n",
    "        x, y = data\n",
    "        ax.scatter(x, y, marker='.', color='black', label='data')\n",
    "    if ax_labels:\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.set_ylabel('$t$')\n",
    "    if legend: ax.legend()\n",
    "    ax.margins(x=0)\n",
    "\n",
    "# Visualize the data and true function\n",
    "plot_function(f, -0.6, 0.6, data=make_additive_noise_data(200, f, -0.6, 0.6, random_state=42))\n",
    "plt.title('Synthetic Data with True Function $f(x) = \\\\frac{\\\\sin(2\\\\pi x)}{2+3x}$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5eeed",
   "metadata": {},
   "source": [
    "### Step 2: Ridge Regression Training and Regularization Analysis\n",
    "In this step, we train ridge regression models with different regularization parameters (λ) and analyze their effects on model performance and weight norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b149cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# generate lambda values: 10^(-10+9i/100) for i = 0 to 100\n",
    "lambda_values = np.geomspace(10 ** (-10), 0.1, 101, endpoint=True)\n",
    "print(f\"Lambda range: {lambda_values[0]:.2e} to {lambda_values[-1]:.2e}\")\n",
    "print(f\"Number of lambda values: {len(lambda_values)}\")\n",
    "\n",
    "# set the number of training repeats\n",
    "n_repeats = 15\n",
    "\n",
    "# variables to store the results of all repeats\n",
    "all_train_errors = []\n",
    "all_test_errors = []\n",
    "\n",
    "# generate the test data\n",
    "X_test, y_test = make_additive_noise_data(5000, f, -0.6, 0.6, random_state=818)\n",
    "\n",
    "print(\"\\nTraining starts...\")\n",
    "\n",
    "for repeat in range(n_repeats):\n",
    "    # generate the training data\n",
    "    X_train, y_train = make_additive_noise_data(20, f, -0.6, 0.6, random_state=818 + repeat)\n",
    "    poly_features = PolynomialFeatures(degree=4, include_bias=True)\n",
    "    # transform the training and test data\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    X_test_poly = poly_features.transform(X_test)\n",
    "    # variables to store the results for each repeat\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for i, lam in enumerate(lambda_values):\n",
    "        # create and train the ridge regression model\n",
    "        model = SGDLinearRegressor(\n",
    "            lam=lam,\n",
    "            eta=0.01,\n",
    "            tau_max=2000,\n",
    "            epsilon=1e-6,\n",
    "            random_state=818 - repeat\n",
    "        )\n",
    "        # fit the model\n",
    "        model.fit(X_train_poly, y_train)\n",
    "        # predict\n",
    "        y_train_pred = model.predict(X_train_poly)\n",
    "        y_test_pred = model.predict(X_test_poly)\n",
    "        # calculate the error\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        # store the results\n",
    "        train_errors.append(train_mse)\n",
    "        test_errors.append(test_mse)\n",
    "\n",
    "        if i == len(lambda_values) - 1:\n",
    "            print(\n",
    "                f\"Training {repeat+1}/{n_repeats}, Train MSE = {train_mse:.4f}, Test MSE = {test_mse:.4f}\"\n",
    "            )\n",
    "\n",
    "    all_train_errors.append(train_errors)\n",
    "    all_test_errors.append(test_errors)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "\n",
    "# calculate the mean and standard deviation of the results\n",
    "mean_train_errors = np.mean(all_train_errors, axis=0)\n",
    "mean_test_errors = np.mean(all_test_errors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2eb277",
   "metadata": {},
   "source": [
    "### Step 3: Results Visualization\n",
    "Plotting the results to illustrate the effect of different λ values on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===== Plot: MSE vs lambda =====\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(\n",
    "    lambda_values,\n",
    "    mean_train_errors,\n",
    "    label=\"Train MSE\",\n",
    "    color=\"blue\",\n",
    "    marker=\"o\",\n",
    "    markersize=3,\n",
    "    linewidth=1,\n",
    ")\n",
    "\n",
    "\n",
    "plt.plot(\n",
    "    lambda_values,\n",
    "    mean_test_errors,\n",
    "    label=\"Test MSE\",\n",
    "    color=\"orange\",\n",
    "    marker=\"s\",\n",
    "    markersize=3,\n",
    "    linewidth=1,\n",
    ")\n",
    "\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.xlabel(\"λ (log scale)\")\n",
    "plt.ylabel(\"Mean Squared Error (log scale)\")\n",
    "plt.title(\"Effect of λ on Ridge Regression (Polynomial degree=4)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\":\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e97738",
   "metadata": {},
   "source": [
    "### Step 4: Analysis\n",
    "blabla~ TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
